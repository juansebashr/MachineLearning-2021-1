{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practice Problems 5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QYy2yrYogGMZ"},"source":["\n","# Practice Problems 5\n","# Machine Learning 2021-1\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"rdX99IbpgGMb"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xY9THq-7gGMj"},"source":["### 1. (2.0)\n","The following code implements a feed-forward neural network:"]},{"cell_type":"code","metadata":{"id":"WuzJ773kgGMl"},"source":["def sigmoid(x):\n","    return 1.0/(1.0 + np.exp(-x))\n","\n","def relu(x):\n","    return max(0, x)\n","\n","def predict(w, x):\n","    '''\n","    w: numpy array of shape(9,)\n","    x: numpy array of shape(2,)\n","    '''\n","    z = np.zeros((3,))\n","    z[0] = relu(x @ w[3:5] + w[0])\n","    z[1] = relu(x @ w[5:7] + w[1])\n","    z[2] = sigmoid(z[0] * w[7] + z[1] * w[8] + w[2])\n","    return z[2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9vlPmRiKgGMq"},"source":["Find a weight vector such that the neural network calculates the following function:\n","    \n","$$f(x_0,x_1)=\\neg x_0 \\lor x_1$$\n","\n","Use the following variable to put the weight vector:"]},{"cell_type":"code","metadata":{"id":"zEMfiASkgGMt"},"source":["W1 = np.zeros(9) # put the values of W1 here\n","W1 = [-11.85744345, -45.63016252,  88.50236842,  13.22427549, -99.09458438, 91.29999204 ,-98.33041628, -43.91063932, -91.8077625]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_d8PcMMfJqbV"},"source":["A picture of the Neural Network implemented is described here, note that $x_1=x[0]$ and $x_2=x[1]$\n","\n","<center><img src=\"https://raw.githubusercontent.com/juansebashr/MachineLearning-2021-1/main/Quices/nn.png\"></center>"]},{"cell_type":"markdown","metadata":{"id":"s504dU-rgGM3"},"source":["### 2. (3.0)\n","\n","Assuming the following loss function:"]},{"cell_type":"code","metadata":{"id":"hPkRws3KgGM5"},"source":["def loss(w, x, y):\n","    '''\n","    w: numpy array of shape(9,)\n","    x: numpy array of shape(2,)\n","    y: scalar float value\n","    '''    \n","    return (y - predict(w, x))**2/2."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JoqNvpBzgGM7"},"source":["Write a function that uses backpropagation to calculate:\n","    \n","$$\\frac{\\partial E(w, x, y)}{\\partial w_5\n","}$$\n","\n","Where $E$ is the loss function defined before. Explicitely write the expressions that you derive:\n","\n","Expressing $h$ as the ReLu function and $h'$ as it derivate we have \n","\n","$$\\delta_1 = h'(a_1)\\sum_{k=1}^{m}\\delta_kw_{kj}=h'(a_1)\\delta_yw_8$$\n","\n","$$\\delta_2 =\\delta_y =\\frac{\\partial E}{\\partial a_2}=\\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial a_2}=(y-r^{\\ell})[\\sigma(a_2)(1-\\sigma(a_2))]$$\n","\n","$$\\frac{\\partial E(w, x, y)}{\\partial w_5} = \\frac{\\partial E}{\\partial a_1} \\frac{\\partial a_1}{\\partial w_5} = \\delta_2 x_1 $$"]},{"cell_type":"markdown","metadata":{"id":"ZMXtCYZuqBPE"},"source":["Here as an academic exercise, $\\partial E/\\partial w_5$ is implemented the \"long way\" for compare with the simplified implmentation as $\\delta_2 x_1$"]},{"cell_type":"code","metadata":{"id":"2zpSi5bXgGM8"},"source":["def dE_dw5(w, x, y):\n","    '''\n","    w: numpy array of shape(9,)\n","    x: numpy array of shape(2,)\n","    y: scalar float value\n","    \n","    Returns:\n","    (d1, d2, de): a tuple with three scalar values delta_1 (d1), delta_2 (d2) and the derivative (de)\n","    '''    \n","    z = np.zeros((3,))\n","    z[0] = relu(x @ w[3:5] + w[0]) #z of the first neuron of the hidden layer of the network\n","    z[1] = relu(x @ w[5:7] + w[1]) #z of the second neuron of the hidden layer of the network\n","    a0 = x @ w[3:5] + w[0] #a of the first neuron\n","    a1 = x @ w[5:7] + w[1]   #a of the second neuron \n","    aY = z[0] * w[7] + z[1] * w[8] + w[2] #a from the output neuron\n","    Dz1Dw5 = DreluDXfun(a1, x[0])\n","    DaYDz1 = w[8]\n","    DyDaY = (np.exp(-aY)) / ((1 + np.exp(-aY))**2)\n","    DyDz1 = DyDaY * DaYDz1\n","    DyDw5 = DyDz1 * Dz1Dw5\n","    dEDy = -(y - predict(w,x)) \n","    dEDw5 = dEDy * DyDw5\n","    deltay = dEDy * DyDaY\n","    delta1= drelu(aY)*w[8]*dEDy*DyDaY\n","    print(np.round(dEDw5,5)==np.round(delta1*x[0],5))\n","    return (np.array(delta1),np.array(deltay),dEDw5)\n","\n","def drelu(x):\n","  return 0 if x < 0 else 1\n","\n","def DreluDXfun(B, x0):\n","  if B <= 0:\n","    return 0\n","  else:\n","    return x0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiJmCiaIAdLc"},"source":["w=np.array([-0.86956799, 1.7346629,  -0.21577933,  0.09178123,  0.08547155,  1.62031338, -0.92339911, -0.45251222,  0.60249779])\n","x=np.array([0.87776175, 0.62685012])\n","y=np.array([0.08359318])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXJyVTBOBS2X","executionInfo":{"status":"ok","timestamp":1620419043127,"user_tz":300,"elapsed":12,"user":{"displayName":"Juan Sebastian Hernandez Reyes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicRVSz4fLji_lex7ou2O8Kykhjcn4QADUZZcZgMg=s64","userId":"03703498159372912448"}},"outputId":"09fcc0f4-b7cd-4b19-bee0-3dc5be1fe5aa"},"source":["print(dE_dw5(w,x,y))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ True]\n","(array([0.07029958]), array([0.11668023]), array([0.06170629]))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MjaOx2_7f-qb"},"source":[""],"execution_count":null,"outputs":[]}]}